# Assignment 2 - AR Foundation basics (ungraded)

**Date**: 11/02/2022

**Group members participating**: Rasmus Thors√∏e, Sam Martin Vargas Giagnocavo and Nishka Dasgupta.

**Activity duration**: 7 to 8 hours.

## Goal
- Begin building a phone app.
- Learn basic Unity functionality such as scene creation, plane detection, object tracking.
- Create a mobile app that can add an object tethered to a plane in the physical world.

## Plan
As in the previous week, we initially decided that each member should work on all exercises independently. So over the course of the week we each worked independently on the remaining exercises and made different degrees of progress. On Friday we decided to split the remaining coding work between Sam and Rasmus, with Nishka and Sam working on the report.

## Results
Exercise 1 was completed by all of us in the TA session... And we did make sure that our camera was tagged as *MainCamera*:
![img](media/assignment_2/camera.png)


### <ins>Exercise 1.2</ins>
After building the application as a phone app, it shows approximate planes being tracked in the physical world, e.g a bed or a windowsill. In low light conditions it does not do well with planes that are close to the camera, e.g the laptop keyboard a few inches away. On occasion the application will appear to track planes where none exist, e.g in mid-air.

### <ins>Exercise 2.1</ins>
For this exercise we created a cube object with the given parameters (side length 0.2, rigidbody with gravity enabled). We then made it a prefab and deleted the object.

To retrieve the camera that we use in the scene we used `Camera camera = Camera.main` as opossed to what we did in the previous assignment which was select our camera from a `[SerializeField]`.

The cube object appears to have sides of length slightly less than 30 cm, when compared against various objects in the physical world, both near and far from the camera. As the side of the cube prefab was set to 0.2 units in unity, we can assume that 1 unit in unity corresponds to a little over 1 metre in the physical world.

Two implementations were made when positioning the cubes. In the first one, no offset is added to the cubes position, so when we enter a cube at the same position as an existing cube, the cubes explosively repel each other and are flung to different corners of the space.

The second, added a small offset to the cubes Y-axis. In this case, the cubes collide as seen in the GIF below.

![img](media/assignment_2/ex2.1-default-plane-cubes.gif)


### <ins>Exercise 2.2</ins>
Once we have deleted the `AR Default Plane` that was previously added, we implemnented AR Foundations' raycast function. Similar to what we did in the previous assignment, we placed a conditional inside the `Update` function that would register the user's touch and check whether there was an invisible AR plane in the way.

```c#
[SerializeField]
ARRaycastManager raycastManager;

// Code...

void Update()
{
    
    Ray ray;
    List<ARRaycastHit> hits = new List<ARRaycastHit>();
    
    // placePos = get position that was touched...
    
    ray = camera.ScreenPointToRay(placePos);

    if (raycastManager.Raycast(placePos, hits)) {
        raycastPos = ray.GetPoint(hits[0].distance);
        
        // Magic...
    }

}
```

Using the library's raycast function we observe that now new objects do appear, but they fall through the ground. This is due to the objects having gravity enabled and not being another object (with a collider) to stop them from falling (that was the function of the `AR Default Plane`). An example of the cubes endlessly falling can be seen below.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex2.2-fall-through-processed.mp4" type="video/mp4">
  </video>
</figure>

To fix this we remove gravity from the prefab's *Rigidbody*. This is the result: 

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex2.2-collide-wout-gravity-processed.mp4" type="video/mp4">
  </video>
</figure>

Since the exercise did not specify whether the object's *Collider* had to be removed or not, we decided not to remove it. That's why the objects repel each other when positioned in the same coodinates.

Explain the difference between the two different raycast functions (Unity's build-in and AR Foundation's)?
- When will you use each function?
- Can you combine them in a meaningful way?
- If you want to use gravity in your project, how can you change the design of the debug planes to look nicer?]

In this exercise, we began to use the AR Foundation raycast. This initially caused our cubes to fall through the ground. The gravity feature of the cube prefab was enabled. Further, within the rigidbody feature, collision detection was set to discrete, i.e, at specific discrete time intervals, the object would move a fixed number of units, according to the force applied on it (in this case, gravity). This meant that the game may not be able to detect the collision at all since the cube only changed position directly without passing through intermediate points (the ground). On disabling rigidbody, the object no longer fall through the ground.

Now, on adding an object on top of the current object, the new object simply applies just in front of the old object. 

The Unity built-in raycast function tracks objects in the physical world, whereas AR Foundation's raycast tracks trackable AR objects that may not exist in the physical world.

### <ins>Exercise 2.3</ins>
Once a texture for the indication marker was found, we added it to the textures folder.

To display the marker every time AR Foundation detects a surface, we trigger a ray for each update. With `ViewportToScreenPoint` we get the position of the center of the screen. We then use that position for the `Raycast` function from `ARRaycastManager`.

```c#
void Update()
{
    
    Ray ray;
    List<ARRaycastHit> hits = new List<ARRaycastHit>();
    
    // SHOULD WE POSITION THE PLACEHOLDER ?
    placePos = camera.ViewportToScreenPoint(new Vector3(0.5f, 0.5f, 0));
    ray = camera.ScreenPointToRay(placePos);

    if (raycastManager.Raycast(placePos, hits)) {
        raycastPos = ray.GetPoint(hits[0].distance);
        if (activePlaceholder) {
            activePlaceholder.transform.position = raycastPos;
        } else {
            activePlaceholder = Instantiate(placeholder, raycastPos, Quaternion.identity);
        }
    } else if (activePlaceholder) {
        Destroy(activePlaceholder);
    }

}
```

Since the indication marker has to be tied to the center of the screen we change it's position if the object has already been instantiated (we track this with the `activePlaceholder` variable). An example of this behaviour can be seen below.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex2.3-indicator-processed.mp4" type="video/mp4">
  </video>
</figure>

### <ins>Exercise 2.4</ins>
Luckily for us the functionality that is required for this exercise has been already implemented. We only need to move some code around. For this exercise, we move our `Instantiate` call inside of a function called `AddCube`.

Then select the `AddCube` function in the inspector pane of the button we just added to our scene. This is what it looks like (after fiddling a bit with the button's position).

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex2.4-button-processed.mp4" type="video/mp4">
  </video>
</figure>

### <ins>Exercise 2.5</ins>
Since our group hasn't settled for an idea yet, we decided to choose a [random 3D model from the Unity Store](https://assetstore.unity.com/packages/3d/props/bowling-kegel-ball-67371). For the example we will use the AR raycast function since the pins will be placed on top of the "real" floor.

An example can be seen below:
<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex2.5-meaningful-objs-processed.mp4" type="video/mp4">
  </video>
</figure>

### <ins>Exercise 3.1</ins>
Now, our goal is to track predefined images. To achieve that we create a library of target images, a `ReferenceImageLibrary`. For this example we have chosen to take photos of two objects (a book and a brochure) that had enough keypoints to be considered good refence images.

<p float="left">
  <img src="media/assignment_2/refimage-2.jpg" height="150" /> 
  <img src="media/assignment_2/refimage-1.png" height="150" />
</p>

After that, we add an AR Tracked Image Manager component to our AR Session Origin object (just like we did for our AR Raycast Manager) and specify the collection of images we just created.

To not make our original script too long, a new script was created, `TrackedObject.cs`. Here we will subscribe to the `trackedImagesChanged` event:

```c#
void Start()
{
    trackedImageManager.trackedImagesChanged += OnChanged;
}

void OnChanged(ARTrackedImagesChangedEventArgs eventArgs)
{
    foreach (var newImage in eventArgs.added)
    {
        // We want to add as a child so we use newImage.transform
        Instantiate(prefab, new Vector3(0, 0, 0), Quaternion.identity, newImage.transform);
    }

    // More code...
}
```

This will create a meaningful object as soon as it detects an image from our library. The object is tied to the predefined image; so if the image were to be moved, the object would move too. 

### <ins>Exercise 3.2</ins>
For the test, we decided to change the scale of the book and keep the original size of the brochure. This was done in the inspector pane when selecting the Reference Image Library.

![img](media/assignment_2/change-scale.png)

The end result can be seen here:

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="media/assignment_2/ex3.2-diff-scales-ref-imgs-processed.mp4" type="video/mp4">
  </video>
</figure>

Due to the book being registered as twice the size as it really is, Unity thinks the book is further than it really is. That is why the meaningful object appears smaller when compared to the one displayed on top of the brochure.

## Conclusion
During this assignment we learned the basics of mixing the virtual world with the physical world in Unity. These concepts will be vital to our final project.

We also further refined our final project idea.

## References
- [Unity Docs](https://docs.unity3d.com/Manual/index.html)
